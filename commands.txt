python -m pip install -U llama-cpp-python `
  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124

  https://developer.download.nvidia.com/compute/cuda/redist/

  py -3.12 -m pip install --only-binary=:all: --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124 llama-cpp-python==0.3.4
  Available versions: 0.3.4, 0.2.90, 0.2.88, 0.2.87, 0.2.86, 0.2.85, 0.2.84, 0.2.83, 0.2.82, 0.2.81, 0.2.77, 0.2.76, 0.2.75, 0.2.74, 0.2.73, 0.2.72, 0.2.71, 0.2.70, 0.2.69, 0.2.68, 0.2.67, 0.2.66

  py -3.12 -m pip uninstall -y llama-cpp-python  


  C:\Users\userid\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\lib
  C:\Users\userid\AppData\Roaming\Python\Python312\site-packages

python -m pip install -U pip setuptools wheel scikit-build-core
$env:CMAKE_ARGS = "-DLLAMA_CUBLAS=on -DLLAMA_CUDA_F16=on"
$env:FORCE_CMAKE = "1"
python -m pip install --no-binary=:all: "git+https://github.com/abetlen/llama-cpp-python.git@master"

https://ai.azure.com/catalog/publishers/openai


# 0) Clean old installs
py -3.12 -m pip uninstall -y llama-cpp-python

# 1) Make sure CUDA *Toolkit* is installed and on PATH (adjust version/folder if needed)
$env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4"
$env:PATH = "$env:CUDA_PATH\bin;$env:PATH"

# Quick sanity check (these files must exist)
if (!(Test-Path "$env:CUDA_PATH\include\cublas_v2.h")) { Write-Error "Missing Toolkit headers: $env:CUDA_PATH\include\cublas_v2.h"; break }
if (!(Test-Path "$env:CUDA_PATH\lib\x64\cublas.lib")) { Write-Error "Missing Toolkit libs: $env:CUDA_PATH\lib\x64\cublas.lib"; break }

# 2) Tell CMake to build CUDA backend (new flags)
$env:CMAKE_ARGS = "-DGGML_CUDA=on -DGGML_CUDA_F16=on -DGGML_CUDA_TARGET_SM=86 -DGGML_NATIVE=on"
$env:FORCE_CMAKE = "1"

# 3) Build from sdist (0.3.16) so you get the newer llama.cpp core
py -3.12 -m pip install --no-binary=:all: llama-cpp-python==0.3.16

py -3.12 -m pip install --no-binary=:all: "llama-cpp-python @ git+https://github.com/abetlen/llama-cpp-python@main"


# 0) Use the TD Python 3.11
py -3.11 -m pip uninstall -y llama-cpp-python

# 1) Ensure build tooling is present (pure wheels)
py -3.11 -m pip install -U pip setuptools wheel scikit-build-core cmake

# 2) Point to your CUDA Toolkit (adjust if not v12.4)
$env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v13.0"
$env:PATH = "$env:CUDA_PATH\bin;$env:PATH"

# (sanity checks)
if (!(Test-Path "$env:CUDA_PATH\include\cublas_v2.h")) { throw "Missing: $env:CUDA_PATH\include\cublas_v2.h" }
if (!(Test-Path "$env:CUDA_PATH\lib\x64\cublas.lib")) { throw "Missing: $env:CUDA_PATH\lib\x64\cublas.lib" }

# 3) Tell CMake to use CUDA backend (new flags; NOT LLAMA_CUBLAS)
#    Force MSVC generator to avoid ninja completely.
$env:CMAKE_ARGS = "-DGGML_CUDA=on -DGGML_CUDA_F16=on -DGGML_CUDA_TARGET_SM=86 -DGGML_NATIVE=on -G ""Visual Studio 17 2022"""
$env:FORCE_CMAKE = "1"

# 4) Avoid build isolation so pip does NOT try to bring in meson/ninja
$env:PIP_NO_BUILD_ISOLATION = "1"

# 5) Build and install from sdist (has Phi-4 fixes)
py -3.11 -m pip install --no-binary=:all: llama-cpp-python==0.3.16

************************************

# 1) Make sure CUDA 13 is on PATH
$env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v13.0"
$env:PATH = "$env:CUDA_PATH\bin;$env:PATH"

# 2) Tooling
py -3.11 -m pip install -U pip setuptools wheel scikit-build-core cmake

# 3) Install deps as WHEELS (avoid building numpy/typing-extensions)
py -3.11 -m pip install --only-binary=:all: numpy==2.3.4 typing-extensions==4.15.0 diskcache jinja2

# 4) Tell CMake to use MSBuild instead of ninja + enable CUDA
$env:CMAKE_GENERATOR = "Visual Studio 17 2022"
$env:FORCE_CMAKE = "1"
$env:CMAKE_ARGS = "-DGGML_CUDA=on -DGGML_CUDA_F16=on -DGGML_CUDA_TARGET_SM=86 -DGGML_NATIVE=on"

# 5) Now build ONLY llama-cpp-python from source (deps won’t rebuild)
$env:PIP_NO_BUILD_ISOLATION = "1"
py -3.11 -m pip install --no-binary=llama-cpp-python llama-cpp-python==0.3.16


:: 1) Work somewhere writable
mkdir C:\Users\balum\llama_build && cd /d C:\Users\balum\llama_build

:: 2) Tools
py -3.11 -m pip install -U pip cmake scikit-build-core setuptools wheel

:: 3) Point to CUDA 12.x on your dev box (match TD’s CUDA 12)
set "CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4"
set "PATH=%CUDA_PATH%\bin;%PATH%"

:: 4) Enable CUDA and target A2000 (SM 86)
set "CMAKE_ARGS=-DGGML_CUDA=on -DGGML_CUDA_F16=on -DGGML_CUDA_TARGET_SM=86 -DGGML_NATIVE=on"

:: 5) Build the wheel (from source), output to wheels\
py -3.11 -m pip wheel --no-deps --no-build-isolation --no-binary=llama-cpp-python ^
  llama-cpp-python==0.3.16 -w .\wheels
